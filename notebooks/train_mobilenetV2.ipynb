{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac08f37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: imports & config\n",
    "import os, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Config\n",
    "DATA_ROOT = \"../fer2013\"    # change if your extracted folder is elsewhere\n",
    "TRAIN_DIR = os.path.join(DATA_ROOT, \"train\")\n",
    "TEST_DIR  = os.path.join(DATA_ROOT, \"test\")\n",
    "\n",
    "NUM_EPOCHS = 8              # start small; increase if you have time\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 4             # set 0 on Windows if you get issues\n",
    "SEED = 42\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856372a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 28709\n",
      "Test samples: 7178\n",
      "Classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Data transforms and loaders\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=transform_train)\n",
    "test_dataset = datasets.ImageFolder(TEST_DIR, transform=transform_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e701355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model ready for Phase 1 training (classifier only)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load pretrained MobileNetV2 and modify for 7 classes\n",
    "\n",
    "mobilenet = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the final classification layer for 7 emotion classes\n",
    "num_ftrs = mobilenet.classifier[1].in_features\n",
    "mobilenet.classifier[1] = nn.Linear(num_ftrs, 7)\n",
    "\n",
    "mobilenet = mobilenet.to(device)\n",
    "\n",
    "# # Phase 1: Freeze all feature extractor layers (only train classifier)\n",
    "# for param in mobilenet.features.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mobilenet.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "print(\"✅ Model ready for Phase 1 training (classifier only)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c826fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] | Train Loss: 1.2109 | Val Loss: 1.0301 | Train Acc: 54.05% | Val Acc: 60.74%\n",
      "Epoch [2/10] | Train Loss: 0.9502 | Val Loss: 0.9701 | Train Acc: 64.27% | Val Acc: 63.33%\n",
      "Epoch [3/10] | Train Loss: 0.8314 | Val Loss: 0.9396 | Train Acc: 69.00% | Val Acc: 65.06%\n",
      "Epoch [4/10] | Train Loss: 0.7341 | Val Loss: 0.9265 | Train Acc: 73.15% | Val Acc: 66.45%\n",
      "Epoch [5/10] | Train Loss: 0.6284 | Val Loss: 0.9904 | Train Acc: 77.02% | Val Acc: 65.77%\n",
      "Epoch [6/10] | Train Loss: 0.4545 | Val Loss: 0.9673 | Train Acc: 84.55% | Val Acc: 67.41%\n",
      "Epoch [7/10] | Train Loss: 0.3959 | Val Loss: 0.9953 | Train Acc: 86.75% | Val Acc: 67.15%\n",
      "Epoch [8/10] | Train Loss: 0.3639 | Val Loss: 1.0189 | Train Acc: 88.19% | Val Acc: 67.00%\n",
      "Epoch [9/10] | Train Loss: 0.3302 | Val Loss: 1.0536 | Train Acc: 89.16% | Val Acc: 67.47%\n",
      "Epoch [10/10] | Train Loss: 0.3066 | Val Loss: 1.0795 | Train Acc: 90.04% | Val Acc: 67.08%\n",
      "\n",
      "✅ Training complete! Best Val Accuracy: 67.47%\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Training setup and loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mobilenet.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "EPOCHS = 10  # Increase if GPU allows\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    mobilenet.train()\n",
    "    train_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mobilenet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Validation step\n",
    "    mobilenet.eval()\n",
    "    val_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = mobilenet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_acc = 100 * correct / total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] | \"\n",
    "          f\"Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "          f\"Val Loss: {val_loss/len(test_loader):.4f} | \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(mobilenet.state_dict(), \"best_mobilenetv2.pth\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(f\"\\n✅ Training complete! Best Val Accuracy: {best_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca6f158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuned MobileNetV2 saved successfully at: ../trained_models/mobilenetv2_finetuned.pth\n"
     ]
    }
   ],
   "source": [
    "# ✅ Cell 6: Save the fine-tuned MobileNetV2 model\n",
    "save_path = \"../trained_models/mobilenetv2_finetuned.pth\"\n",
    "\n",
    "torch.save(mobilenet.state_dict(), save_path)\n",
    "\n",
    "print(f\"✅ Fine-tuned MobileNetV2 saved successfully at: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78031055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
